{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subodh_pushkar/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/subodh_pushkar/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "#import warnings\n",
    "#warnings.simplefilter('ignore')\n",
    "import scipy as sp\n",
    "import scipy.ndimage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "import skimage.exposure\n",
    "# import mahotas as mh\n",
    "from sklearn.cross_validation import KFold\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import h5py\n",
    "from tqdm import tqdm_notebook\n",
    "from IPython.display import display\n",
    "# from dual_IDG import DualImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, \\\n",
    "    Convolution2D, MaxPooling2D, ZeroPadding2D, Input, Embedding, LSTM, merge, \\\n",
    "    Lambda, UpSampling2D, Deconvolution2D, Cropping2D\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, CSVLogger\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_IOU_gpu(X, Y):\n",
    "    \"\"\"Computes mean Intersection-over-Union (IOU) for two arrays of binary images.\n",
    "    Assuming X and Y are of shape (n_images, w, h).\"\"\"\n",
    "    \n",
    "    #X_fl = K.clip(K.batch_flatten(X), K.epsilon(), 1.)\n",
    "    #Y_fl = K.clip(K.batch_flatten(Y), K.epsilon(), 1.)\n",
    "    X_fl = K.clip(K.batch_flatten(X), 0., 1.)\n",
    "    Y_fl = K.clip(K.batch_flatten(Y), 0., 1.)\n",
    "    X_fl = K.cast(K.greater(X_fl, 0.5), 'float32')\n",
    "    Y_fl = K.cast(K.greater(Y_fl, 0.5), 'float32')\n",
    "\n",
    "    intersection = K.sum(X_fl * Y_fl, axis=1)\n",
    "    union = K.sum(K.maximum(X_fl, Y_fl), axis=1)\n",
    "    # if union == 0, it follows that intersection == 0 => score should be 0.\n",
    "    union = K.switch(K.equal(union, 0), K.ones_like(union), union)\n",
    "    return K.mean(intersection / K.cast(union, 'float32'))\n",
    "\n",
    "\n",
    "def mean_IOU_gpu_loss(X, Y):\n",
    "    return -mean_IOU_gpu(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(y_true, y_pred):\n",
    "    # Workaround for shape bug. For some reason y_true shape was not being set correctly\n",
    "    #y_true.set_shape(y_pred.get_shape())\n",
    "\n",
    "    # Without K.clip, K.sum() behaves differently when compared to np.count_nonzero()\n",
    "    #y_true_f = K.clip(K.batch_flatten(y_true), K.epsilon(), 1.)\n",
    "    #y_pred_f = K.clip(K.batch_flatten(y_pred), K.epsilon(), 1.)\n",
    "    y_true_f = K.clip(K.batch_flatten(y_true), 0., 1.)\n",
    "    y_pred_f = K.clip(K.batch_flatten(y_pred), 0., 1.)\n",
    "    #y_pred_f = K.greater(y_pred_f, 0.5)\n",
    "\n",
    "    intersection = 2 * K.sum(y_true_f * y_pred_f, axis=1)\n",
    "    union = K.sum(y_true_f * y_true_f, axis=1) + K.sum(y_pred_f * y_pred_f, axis=1)\n",
    "    return K.mean(intersection / union)\n",
    "\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return -dice(y_true, y_pred)\n",
    "\n",
    "\n",
    "def log_dice_loss(y_true, y_pred):\n",
    "    return -K.log(dice(y_true, y_pred))\n",
    "\n",
    "\n",
    "def dice_metric(y_true, y_pred):\n",
    "    \"\"\"An exact Dice score for binary tensors.\"\"\"\n",
    "    y_true_f = K.cast(K.greater(y_true, 0.5), 'float32')\n",
    "    y_pred_f = K.cast(K.greater(y_pred, 0.5), 'float32')\n",
    "    return dice(y_true_f, y_pred_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "from losses import *\n",
    "\n",
    "def get_unet_128(input_shape=(128, 128, 3),\n",
    "                 num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # 128\n",
    "\n",
    "    down1 = Conv2D(64, (3, 3), padding='same')(inputs)\n",
    "    down1 = BatchNormalization()(down1)\n",
    "    down1 = LeakyReLU(alpha=0.15)(down1)\n",
    "    down1 = Conv2D(64, (3, 3), padding='same')(down1)\n",
    "    down1 = BatchNormalization()(down1)\n",
    "    down1 = LeakyReLU(alpha=0.15)(down1)\n",
    "    down1_pool = MaxPooling2D((2, 2), strides=(2, 2))(down1)\n",
    "    # 64\n",
    "\n",
    "    down2 = Conv2D(128, (3, 3), padding='same')(down1_pool)\n",
    "    down2 = BatchNormalization()(down2)\n",
    "    down2 = LeakyReLU(alpha=0.15)(down2)\n",
    "    down2 = Conv2D(128, (3, 3), padding='same')(down2)\n",
    "    down2 = BatchNormalization()(down2)\n",
    "    down2 = LeakyReLU(alpha=0.15)(down2)\n",
    "    down2_pool = MaxPooling2D((2, 2), strides=(2, 2))(down2)\n",
    "    # 32\n",
    "\n",
    "    down3 = Conv2D(256, (3, 3), padding='same')(down2_pool)\n",
    "    down3 = BatchNormalization()(down3)\n",
    "    down3 = LeakyReLU(alpha=0.15)(down3)\n",
    "    down3 = Conv2D(256, (3, 3), padding='same')(down3)\n",
    "    down3 = BatchNormalization()(down3)\n",
    "    down3 = LeakyReLU(alpha=0.15)(down3)\n",
    "    down3_pool = MaxPooling2D((2, 2), strides=(2, 2))(down3)\n",
    "    # 16\n",
    "\n",
    "    down4 = Conv2D(512, (3, 3), padding='same')(down3_pool)\n",
    "    down4 = BatchNormalization()(down4)\n",
    "    down4 = LeakyReLU(alpha=0.15)(down4)\n",
    "    down4 = Conv2D(512, (3, 3), padding='same')(down4)\n",
    "    down4 = BatchNormalization()(down4)\n",
    "    down4 = LeakyReLU(alpha=0.15)(down4)\n",
    "    down4_pool = MaxPooling2D((2, 2), strides=(2, 2))(down4)\n",
    "    # 8\n",
    "\n",
    "    center = Conv2D(1024, (3, 3), padding='same')(down4_pool)\n",
    "    center = BatchNormalization()(center)\n",
    "    center = LeakyReLU(alpha=0.15)(center)\n",
    "    center = Conv2D(1024, (3, 3), padding='same')(center)\n",
    "    center = BatchNormalization()(center)\n",
    "    center = LeakyReLU(alpha=0.15)(center)\n",
    "    # center\n",
    "\n",
    "    up4 = UpSampling2D((2, 2))(center)\n",
    "    up4 = concatenate([down4, up4], axis=3)\n",
    "    up4 = Conv2D(512, (3, 3), padding='same')(up4)\n",
    "    up4 = BatchNormalization()(up4)\n",
    "    up4 = LeakyReLU(alpha=0.15)(up4)\n",
    "    up4 = Conv2D(512, (3, 3), padding='same')(up4)\n",
    "    up4 = BatchNormalization()(up4)\n",
    "    up4 = LeakyReLU(alpha=0.15)(up4)\n",
    "    up4 = Conv2D(512, (3, 3), padding='same')(up4)\n",
    "    up4 = BatchNormalization()(up4)\n",
    "    up4 = Activation('relu')(up4)\n",
    "    # 16\n",
    "\n",
    "    up3 = UpSampling2D((2, 2))(up4)\n",
    "    up3 = concatenate([down3, up3], axis=3)\n",
    "    up3 = Conv2D(256, (3, 3), padding='same')(up3)\n",
    "    up3 = BatchNormalization()(up3)\n",
    "    up3 = LeakyReLU(alpha=0.15)(up3)\n",
    "    up3 = Conv2D(256, (3, 3), padding='same')(up3)\n",
    "    up3 = BatchNormalization()(up3)\n",
    "    up3 = LeakyReLU(alpha=0.15)(up3)\n",
    "    up3 = Conv2D(256, (3, 3), padding='same')(up3)\n",
    "    up3 = BatchNormalization()(up3)\n",
    "    up3 = LeakyReLU(alpha=0.15)(up3)\n",
    "    # 32\n",
    "\n",
    "    up2 = UpSampling2D((2, 2))(up3)\n",
    "    up2 = concatenate([down2, up2], axis=3)\n",
    "    up2 = Conv2D(128, (3, 3), padding='same')(up2)\n",
    "    up2 = BatchNormalization()(up2)\n",
    "    up2 = LeakyReLU(alpha=0.15)(up2)\n",
    "    up2 = Conv2D(128, (3, 3), padding='same')(up2)\n",
    "    up2 = BatchNormalization()(up2)\n",
    "    up2 = LeakyReLU(alpha=0.15)(up2)\n",
    "    up2 = Conv2D(128, (3, 3), padding='same')(up2)\n",
    "    up2 = BatchNormalization()(up2)\n",
    "    up2 = LeakyReLU(alpha=0.15)(up2)\n",
    "    # 64\n",
    "\n",
    "    up1 = UpSampling2D((2, 2))(up2)\n",
    "    up1 = concatenate([down1, up1], axis=3)\n",
    "    up1 = Conv2D(64, (3, 3), padding='same')(up1)\n",
    "    up1 = BatchNormalization()(up1)\n",
    "    up1 = LeakyReLU(alpha=0.15)(up1)\n",
    "    up1 = Conv2D(64, (3, 3), padding='same')(up1)\n",
    "    up1 = BatchNormalization()(up1)\n",
    "    up1 = LeakyReLU(alpha=0.15)(up1)\n",
    "    up1 = Conv2D(64, (3, 3), padding='same')(up1)\n",
    "    up1 = BatchNormalization()(up1)\n",
    "    up1 = LeakyReLU(alpha=0.15)(up1)\n",
    "    # 128\n",
    "\n",
    "    classify = Conv2D(num_classes, (1, 1), activation='sigmoid')(up1)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=classify)\n",
    "\n",
    "    model.compile(optimizer=RMSprop(lr=0.0001), loss=log_dice_loss,\n",
    "              metrics=[mean_IOU_gpu, dice_metric])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 64) 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 128, 128, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 64) 36928       leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128, 128, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 128, 128, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)   0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 128)  73856       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 128)  512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 64, 64, 128)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 128)  147584      leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 64, 128)  512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 64, 64, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 128)  0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 256)  295168      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 256)  1024        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 32, 32, 256)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 256)  590080      leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 256)  1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 32, 32, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 256)  0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 512)  1180160     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 512)  2048        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 16, 16, 512)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 512)  2359808     leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 512)  2048        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 16, 16, 512)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 512)    0           leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 1024)   4719616     max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 1024)   4096        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 8, 8, 1024)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 8, 8, 1024)   9438208     leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 8, 8, 1024)   4096        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 8, 8, 1024)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 16, 16, 1024) 0           leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 16, 16, 1536) 0           leaky_re_lu_8[0][0]              \n",
      "                                                                 up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 512)  7078400     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 512)  2048        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 16, 16, 512)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 512)  2359808     leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 512)  2048        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 16, 16, 512)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 512)  2359808     leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 512)  2048        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 16, 16, 512)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 32, 32, 512)  0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 768)  0           leaky_re_lu_6[0][0]              \n",
      "                                                                 up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 256)  1769728     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 32, 32, 256)  1024        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 32, 32, 256)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 256)  590080      leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 32, 32, 256)  1024        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 32, 32, 256)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 256)  590080      leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 256)  1024        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 32, 32, 256)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 64, 64, 256)  0           leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 64, 64, 384)  0           leaky_re_lu_4[0][0]              \n",
      "                                                                 up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 64, 64, 128)  442496      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 64, 64, 128)  512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 64, 64, 128)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 64, 64, 128)  147584      leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 64, 64, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 64, 64, 128)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 64, 64, 128)  147584      leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 64, 64, 128)  512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 64, 64, 128)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 128, 128, 128 0           leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 128, 128, 192 0           leaky_re_lu_2[0][0]              \n",
      "                                                                 up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 128, 128, 64) 110656      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 128, 128, 64) 256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 128, 128, 64) 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 128, 128, 64) 36928       leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 128, 128, 64) 256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 128, 128, 64) 0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 128, 128, 64) 36928       leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 128, 128, 64) 256         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 128, 128, 64) 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 128, 128, 1)  65          leaky_re_lu_21[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 34,540,737\n",
      "Trainable params: 34,527,041\n",
      "Non-trainable params: 13,696\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_unet_128()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from PIL import *\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "filelist = glob.glob('dataset/crop/RIM-ONEv2/images/*.jpg')              #change '/'\n",
    "X = np.array([np.array(Image.open(fname)) for fname in filelist])\n",
    "filelist = glob.glob('dataset/crop/RIM-ONEv1/images/*.jpg')              #change '/'\n",
    "X = np.concatenate((X,[np.array(Image.open(fname)) for fname in filelist]),axis=0)\n",
    "filelist = glob.glob('dataset/crop/DRIONS_DB/images/*.jpg')              #change '/'\n",
    "X = np.concatenate((X,[np.array(Image.open(fname)) for fname in filelist]),axis=0)\n",
    "filelist = glob.glob('dataset/crop/DRISHTI_GS/images/*.jpg')              #change '/'\n",
    "X = np.concatenate((X,[np.array(Image.open(fname)) for fname in filelist]),axis=0)\n",
    "filelist = glob.glob('dataset/crop/RIM-ONEv3/images/*.jpg')              #change '/'\n",
    "X = np.concatenate((X,[np.array(Image.open(fname)) for fname in filelist]),axis=0)\n",
    "# X=X.reshape(841,3,128,128)\n",
    "print(X.shape)\n",
    "# X = np.concatenate((glu,nonglu),axis=0)\n",
    "# print(X.shape)\n",
    "# print(X[1,:].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "filelist = glob.glob('dataset/crop/RIM-ONEv2/discs/*.png')              #change '/'\n",
    "Y = np.array([np.array(Image.open(fname)) for fname in filelist])\n",
    "filelist = glob.glob('dataset/crop/RIM-ONEv1/discs/*.png')              #change '/'\n",
    "Y = np.concatenate((Y,[np.array(Image.open(fname)) for fname in filelist]),axis=0)\n",
    "filelist = glob.glob('dataset/crop/DRIONS_DB/discs/*.png')              #change '/'\n",
    "Y = np.concatenate((Y,[np.array(Image.open(fname)) for fname in filelist]),axis=0)\n",
    "filelist = glob.glob('dataset/crop/DRISHTI_GS/discs/*.png')              #change '/'\n",
    "Y = np.concatenate((Y,[np.array(Image.open(fname)) for fname in filelist]),axis=0)\n",
    "filelist = glob.glob('dataset/crop/RIM-ONEv3/discs/*.png')              #change '/'\n",
    "Y = np.concatenate((Y,[np.array(Image.open(fname)) for fname in filelist]),axis=0)\n",
    "Y = Y.reshape(937,128,128,1)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subodh_pushkar/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#Normalization\n",
    "\n",
    "for i in range(937):\n",
    "    X[i,:,:,:] = (X[i,:,:,:] - np.min(X[i,:,:,:])) / (np.max(X[i,:,:,:]) - np.min(X[i,:,:,:]))\n",
    "    Y[i,:,:,:] = (Y[i,:,:,:] - np.min(Y[i,:,:,:])) / (np.max(Y[i,:,:,:]) - np.min(Y[i,:,:,:]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(650, 128, 128, 3) (144, 128, 128, 3) (650, 128, 128, 1) (144, 128, 128, 1) (143, 128, 128, 3) (143, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "#SplittingTheData\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.20, random_state=42)\n",
    "# print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.153, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.18, random_state=1)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape,X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subodh_pushkar/anaconda3/lib/python3.5/site-packages/keras/callbacks.py:999: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "#CallBackFunc\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "callbacks = [EarlyStopping(monitor='val_loss',\n",
    "                           patience=8,\n",
    "                           verbose=1,\n",
    "                           min_delta=1e-4),\n",
    "             ReduceLROnPlateau(monitor='val_loss',\n",
    "                               factor=0.1,\n",
    "                               patience=4,\n",
    "                               verbose=1,\n",
    "                               epsilon=1e-4),\n",
    "             ModelCheckpoint(monitor='val_loss',\n",
    "                             filepath='weights/best_weights.hdf5',\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True),\n",
    "TensorBoard(log_dir='logs')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.load_weights('weights/best_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 650 samples, validate on 143 samples\n",
      "Epoch 1/20\n",
      "650/650 [==============================] - 945s 1s/step - loss: 0.1403 - mean_IOU_gpu: 0.7364 - dice_metric: 0.8386 - val_loss: 0.1532 - val_mean_IOU_gpu: 0.7226 - val_dice_metric: 0.8294\n",
      "Epoch 2/20\n",
      "650/650 [==============================] - 938s 1s/step - loss: 0.1386 - mean_IOU_gpu: 0.7385 - dice_metric: 0.8403 - val_loss: 0.1455 - val_mean_IOU_gpu: 0.7266 - val_dice_metric: 0.8323\n",
      "Epoch 3/20\n",
      "650/650 [==============================] - 938s 1s/step - loss: 0.1378 - mean_IOU_gpu: 0.7388 - dice_metric: 0.8407 - val_loss: 0.1477 - val_mean_IOU_gpu: 0.7199 - val_dice_metric: 0.8282\n",
      "Epoch 4/20\n",
      "650/650 [==============================] - 937s 1s/step - loss: 0.1377 - mean_IOU_gpu: 0.7386 - dice_metric: 0.8404 - val_loss: 0.1485 - val_mean_IOU_gpu: 0.7274 - val_dice_metric: 0.8325\n",
      "Epoch 5/20\n",
      "650/650 [==============================] - 935s 1s/step - loss: 0.1378 - mean_IOU_gpu: 0.7388 - dice_metric: 0.8405 - val_loss: 0.1447 - val_mean_IOU_gpu: 0.7260 - val_dice_metric: 0.8317\n",
      "Epoch 6/20\n",
      "650/650 [==============================] - 937s 1s/step - loss: 0.1373 - mean_IOU_gpu: 0.7402 - dice_metric: 0.8413 - val_loss: 0.1481 - val_mean_IOU_gpu: 0.7239 - val_dice_metric: 0.8300\n",
      "Epoch 7/20\n",
      "104/650 [===>..........................] - ETA: 12:17 - loss: 0.1548 - mean_IOU_gpu: 0.7186 - dice_metric: 0.8250"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "H = model.fit(X_train, y_train, batch_size=8, epochs=epochs, verbose=1,\n",
    "              callbacks=callbacks,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train, y_train, epochs=1, batch_size=1,shuffle=True,callbacks=callbacks,validation_data=(X_val, y_val))\n",
    "scores = model.evaluate(X_test, y_test, batch_size=1, steps=None)\n",
    "print(scores)\n",
    "print('loss=',scores[0])\n",
    "print('metric',scores[1],scores[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Creates a HDF5 file 'my_model.h5'\n",
    "model.save('models/second_model.h5')\n",
    "\n",
    "# Deletes the existing model\n",
    "del model  \n",
    "\n",
    "# Returns a compiled model identical to the previous one\n",
    "model = load_model('models/second_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(H.history['loss'])\n",
    "plt.plot(H.history['val_loss'])\n",
    "plt.title('Training for ' +str(10)+ ' epochs')\n",
    "plt.legend(['Training Loss', 'Validation Loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(H.history['acc'])\n",
    "plt.plot(H.history['val_acc'])\n",
    "plt.title('Training for ' +str(10)+ ' epochs')\n",
    "plt.legend(['Training accuracy', 'Validation accuracy'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
